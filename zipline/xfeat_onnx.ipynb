{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzFYZYcT9oyb"
      },
      "source": [
        "# XFeat matching example (sparse and semi-dense)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgPDBaVw9uSU"
      },
      "source": [
        "## First, clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4G-r76a9tfR",
        "outputId": "940c2226-a4b4-4c30-a6f6-6fc93ba76eff"
      },
      "outputs": [],
      "source": [
        "# !cd /content && git clone 'https://github.com/verlab/accelerated_features.git'\n",
        "%cd /Users/vaidehi.som/Documents/accelerated_features\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Mbt4a89z3Z"
      },
      "source": [
        "## Initialize XFeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIl4kgXY9zFB",
        "outputId": "6a4f7e7f-85dc-4904-ad57-5a92a0e28f0d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.abspath('..'))  # Add parent directory to Python path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio as imio\n",
        "import time\n",
        "import onnxruntime as ort\n",
        "from PIL import Image\n",
        "\n",
        "from modules.xfeat_match_star import XFeat\n",
        "\n",
        "xfeat_model = XFeat()\n",
        "xfeat_model.eval()\n",
        "\n",
        "#Load some example images\n",
        "im1 = np.copy(imio.v2.imread('../frames/vaidehi_paraglider_multi_object.png')[..., ::-1])\n",
        "im2 = np.copy(imio.v2.imread('../frames/vaidehi_paraglider_multi_object2.png')[..., ::-1])\n",
        "print(im1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_detect_input(im):\n",
        "    im_np = np.array(im, dtype=np.float32)\n",
        "\n",
        "    # Convert RGB to Grayscale using luminosity formula\n",
        "    im_gray = 0.299 * im_np[:,:,0] + 0.587 * im_np[:,:,1] + 0.114 * im_np[:,:,2]\n",
        "\n",
        "    # Convert the numpy grayscale image to a PyTorch tensor\n",
        "    im_tensor = torch.from_numpy(im_gray).unsqueeze(0).unsqueeze(0)  # Adds batch and channel dimensions\n",
        "\n",
        "    return im_tensor \n",
        "\n",
        "def convert_img(im):\n",
        "    return cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mask input images according to given bounding boxes\n",
        "def mask_image(image, bboxes):\n",
        "    \"\"\"\n",
        "    Masks the given image using the bounding boxes provided.\n",
        "    \n",
        "    Parameters:\n",
        "    - image: PyTorch tensor of shape (C, H, W)\n",
        "    - bboxes: List of bounding boxes, each defined as a list of 4 points [top-left, top-right, bottom-left, bottom-right]\n",
        "              Each point is a list [x, y]\n",
        "    \n",
        "    Returns:\n",
        "    - Masked image as a PyTorch tensor\n",
        "    \"\"\"\n",
        "    # Clone the image to avoid modifying the original image\n",
        "    masked_image = torch.zeros_like(image)\n",
        "    # print(masked_image.shape)\n",
        "    # print(bboxes)\n",
        "    \n",
        "    # Process each bounding box\n",
        "    for box in bboxes:\n",
        "        # print(\"box: \", box)\n",
        "        # Extract coordinates\n",
        "        xs = [point[0] for point in box]\n",
        "        ys = [point[1] for point in box]\n",
        "        # print(\"xs: \", xs)\n",
        "        # Determine the bounding rectangle\n",
        "        x_min, x_max = min(xs), max(xs)\n",
        "        y_min, y_max = min(ys), max(ys)\n",
        "        # print(\"x_min: \", x_min)\n",
        "        \n",
        "        # Mask the area\n",
        "        # print(f\"Value before mask at (1,2) in first channel: {masked_image[0, 0, 1, 2]}\")\n",
        "        masked_image[..., y_min:y_max, x_min:x_max] = image[..., y_min:y_max, x_min:x_max]\n",
        "        # print(f\"Value after mask at (1,2) in first channel: {masked_image[0, 0, 1, 2]}\")\n",
        "    \n",
        "    return masked_image\n",
        "    \n",
        "\n",
        "im1_box = [\n",
        "    [[2516, 1063], [2636, 1063], [2516, 1179], [2636, 1179]],\n",
        "    [[2831, 1002], [2877, 1002], [2831, 1044], [2877, 1044]],\n",
        "    [[2903, 725], [2918, 725], [2903, 733], [2918, 733]]\n",
        "]\n",
        "im2_box = [\n",
        "    [[2540, 1083], [2660, 1083], [2540, 1200], [2660, 1200]],\n",
        "    [[2853, 1020], [2897, 1020], [2853, 1064], [2897, 1064]],\n",
        "    [[2937, 734], [2950, 734], [2937, 742], [2950, 742]]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im_gray = prepare_detect_input(im1)\n",
        "\n",
        "# Define a wrapper module for detect_xfeat\n",
        "class DetectXFeatWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(DetectXFeatWrapper, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def forward(self, img):\n",
        "        return self.model.xfeat_detect(img)\n",
        "\n",
        "detect_xfeat_wrapper = DetectXFeatWrapper(xfeat_model)\n",
        "\n",
        "torch.onnx.export(detect_xfeat_wrapper, \n",
        "                  (im_gray), \n",
        "                  \"/Users/vaidehi.som/github/xfeat_star_detect.onnx\", \n",
        "                  export_params=True, \n",
        "                  opset_version=11,  # ONNX version\n",
        "                  training=torch.onnx.TrainingMode.EVAL,\n",
        "                  do_constant_folding=True,  # Whether to execute constant folding for optimization\n",
        "                  input_names=['img'],  # Names of the inputs\n",
        "                  output_names=['out'],  # Name of the output\n",
        "                  dynamic_axes={'img': {2: 'height', 3: 'width'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "session_detect = ort.InferenceSession(\"/Users/vaidehi.som/github/xfeat_star_detect.onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "im1_gray = prepare_detect_input(im1)\n",
        "# im1_gray_masked = mask_image(im1_gray, im1_box)\n",
        "# print(im1_gray.shape)\n",
        "# print(im1_gray_masked)\n",
        "im1_gray_np = im1_gray.cpu().detach().numpy()\n",
        "\n",
        "inputs = {'img': im1_gray_np}\n",
        "output1 = session_detect.run(None, inputs)\n",
        "\n",
        "print(output1[0].shape)\n",
        "print(output1[1].shape)\n",
        "print(output1[2].shape)\n",
        "\n",
        "\n",
        "# Draw keypoints on the images\n",
        "ref_points = output1[0].squeeze(0)\n",
        "keypoints1 = [cv2.KeyPoint(x=p[0], y=p[1], size=5) for p in ref_points]\n",
        "\n",
        "# Draw keypoints on the image\n",
        "im1_cv = convert_img(im1)\n",
        "im1_keypoints = cv2.drawKeypoints(im1_cv, keypoints1, None, color=(255, 0, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Use Matplotlib to display the image\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(im1_keypoints)\n",
        "plt.axis('off') \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im2_gray = prepare_detect_input(im2)\n",
        "# im2_gray_masked = mask_image(im2_gray, im2_box)\n",
        "im2_gray_np = im2_gray.cpu().detach().numpy()\n",
        "\n",
        "inputs = {'img': im2_gray_np}\n",
        "output2 = session_detect.run(None, inputs)\n",
        "\n",
        "print(output2[0].shape)\n",
        "print(output2[1].shape)\n",
        "print(output2[2].shape)\n",
        "\n",
        "\n",
        "# Draw keypoints on the images\n",
        "dst_points = output2[0].squeeze(0)\n",
        "keypoints2 = [cv2.KeyPoint(x=p[0], y=p[1], size=5) for p in dst_points]\n",
        "\n",
        "# Draw keypoints on the image\n",
        "im2_cv = convert_img(im2)\n",
        "im2_keypoints = cv2.drawKeypoints(im2_cv, keypoints2, None, color=(255, 0, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Use Matplotlib to display the image\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(im2_keypoints)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8dOIGoyCGht"
      },
      "source": [
        "## Matching - Semi-dense setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing input for session prep and inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ids_vec_np = np.random.randn(1, output1[0].shape[1]).astype(np.float32)\n",
        "dec1_ids_np = np.arange(1, output1[0].shape[1] + 1).astype(np.float32).reshape(1, -1)\n",
        "dec1_ids_np = np.expand_dims(dec1_ids_np, axis=-1)\n",
        "\n",
        "dec1_kps_np = output1[0].astype(np.float32)\n",
        "dec1_desc_np = output1[1].astype(np.float32)\n",
        "dec1_sc_np = output1[2].astype(np.float32)\n",
        "dec1_sc_np = np.expand_dims(dec1_sc_np, axis=-1)\n",
        "\n",
        "dec2_ids_np = np.arange(1, output2[0].shape[1] + 1).astype(np.float32).reshape(1, -1)\n",
        "dec2_ids_np = np.expand_dims(dec2_ids_np, axis=-1)\n",
        "\n",
        "dec2_kps_np = output2[0].astype(np.float32)\n",
        "dec2_desc_np = output2[1].astype(np.float32)\n",
        "\n",
        "print(dec1_ids_np.shape)\n",
        "print(dec1_kps_np.shape)\n",
        "print(dec1_desc_np.shape)\n",
        "print(dec1_sc_np.shape)\n",
        "print(dec2_ids_np.shape)\n",
        "print(dec2_kps_np.shape)\n",
        "print(dec2_desc_np.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dec1_ids = torch.from_numpy(dec1_ids_np)\n",
        "dec1_kps = torch.from_numpy(dec1_kps_np)\n",
        "dec1_desc = torch.from_numpy(dec1_desc_np)\n",
        "dec1_sc = torch.from_numpy(dec1_sc_np)\n",
        "\n",
        "dec2_ids = torch.from_numpy(dec2_ids_np)\n",
        "dec2_kps = torch.from_numpy(dec2_kps_np)\n",
        "dec2_desc = torch.from_numpy(dec2_desc_np)\n",
        "\n",
        "\n",
        "# Define a wrapper module for detect_xfeat\n",
        "class MatchXFeatWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(MatchXFeatWrapper, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def forward(self, dec1_ids, dec1_kps, dec1_desc, dec1_scales, dec2_ids, dec2_kps, dec2_desc):\n",
        "       return self.model.match_xfeat_star_onnx(\n",
        "           dec1_ids, dec1_kps, dec1_desc, dec1_scales, \n",
        "           dec2_ids, dec2_kps, dec2_desc)\n",
        "\n",
        "match_xfeat_wrapper = MatchXFeatWrapper(xfeat_model)\n",
        "\n",
        "torch.onnx.export(match_xfeat_wrapper, \n",
        "                  (dec1_ids, dec1_kps, dec1_desc, dec1_sc, \n",
        "                   dec2_ids, dec2_kps, dec2_desc),\n",
        "                  \"/Users/vaidehi.som/github/xfeat_star_match.onnx\", \n",
        "                  export_params=True, \n",
        "                  opset_version=18,  # ONNX version\n",
        "                  do_constant_folding=True,  # Whether to execute constant folding for optimization\n",
        "                  input_names = ['dec1_ids', 'dec1_kps', 'dec1_desc', 'dec1_sc',\n",
        "                                  'dec2_ids', 'dec2_kps', 'dec2_desc'],\n",
        "                  output_names=['ref_idx', 'ref_pnts', 'target_idx', 'target_pnts'], #, 'homographyMatrix'], # Name of the output\n",
        "                #   output_names=['ref_pnts', 'target_pnts', 'idx', 'indices'], #, 'homographyMatrix'], # Name of the output\n",
        "                  dynamic_axes={\n",
        "                        'dec1_ids': {1: 'kps'},\n",
        "                        'dec1_kps': {1: 'kps'},\n",
        "                        'dec1_desc': {1: 'kps'},\n",
        "                        'dec1_sc': {1: 'kps'},\n",
        "                        'dec2_ids': {1: 'kps2'},\n",
        "                        'dec2_kps': {1: 'kps2'},\n",
        "                        'dec2_desc': {1: 'kps2'},\n",
        "                        'ref_idx': {0: 'kps3'},\n",
        "                        'ref_pnts': {0: 'kps3'},\n",
        "                        'target_idx': {0: 'kps3'},\n",
        "                        'target_pnts': {0: 'kps3'},\n",
        "                        # 'indices': {0: 'kps3'},\n",
        "                        # 'homographyMatrix': {0: 'kps3'}\n",
        "                    })  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "session_match = ort.InferenceSession(\"/Users/vaidehi.som/github/xfeat_star_match.onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def warp_corners_and_draw_matches(ref_points, dst_points, img1, img2):\n",
        "    # Calculate the Homography matrix\n",
        "    H, mask = cv2.findHomography(ref_points, dst_points, cv2.USAC_MAGSAC, 3.5, maxIters=1000, confidence=0.999)\n",
        "    mask = mask.flatten() > 0\n",
        "\n",
        "    # Get corners of the first image (image1)\n",
        "    h, w = img1.shape[:2]\n",
        "    corners_img1 = np.array([[0, 0], [w - 1, 0], [w - 1, h - 1], [0, h - 1]], dtype=np.float32).reshape(-1, 1, 2)\n",
        "\n",
        "    # Warp corners to the second image (image2) space using the Homography matrix\n",
        "    warped_corners = cv2.perspectiveTransform(corners_img1, H)\n",
        "\n",
        "    # Draw the warped corners in image2\n",
        "    img2_with_corners = img2.copy()\n",
        "    for i in range(len(warped_corners)):\n",
        "        start_point = tuple(warped_corners[i - 1][0].astype(int))\n",
        "        end_point = tuple(warped_corners[i][0].astype(int))\n",
        "        cv2.line(img2_with_corners, start_point, end_point, (0, 255, 0), 4)  # Green color for corners\n",
        "\n",
        "    # Prepare keypoints from the reference and destination points\n",
        "    keypoints1 = [cv2.KeyPoint(p[0], p[1], 5) for p in ref_points]\n",
        "    keypoints2 = [cv2.KeyPoint(p[0], p[1], 5) for p in dst_points]\n",
        "\n",
        "    # keypoints1 = [cv2.KeyPoint(p[0], p[1], 5) for p in ref_points[mask]]\n",
        "    # keypoints2 = [cv2.KeyPoint(p[0], p[1], 5) for p in dst_points[mask]]\n",
        "    # matches = [cv2.DMatch(i, i, 0) for i in range(len(keypoints1))]\n",
        "\n",
        "    # Prepare matches using the mask to filter inliers\n",
        "    matches = [cv2.DMatch(i, i, 0) for i in range(len(mask)) if mask[i]]\n",
        "\n",
        "    # Create a list of colors, each color for each match\n",
        "    colors = [tuple(np.random.randint(0, 255, 3).tolist()) for _ in matches]\n",
        "\n",
        "    # Draw matches with individual colors\n",
        "    img_matches = img1.copy()\n",
        "    for match, color in zip(matches, colors):\n",
        "        img1_idx = match.queryIdx\n",
        "        img2_idx = match.trainIdx\n",
        "\n",
        "        # Draw circles on the keypoints\n",
        "        x1, y1 = keypoints1[img1_idx].pt\n",
        "        x2, y2 = keypoints2[img2_idx].pt\n",
        "        center1 = (int(x1), int(y1))\n",
        "        center2 = (int(x2), int(y2))\n",
        "\n",
        "        # Draw keypoints\n",
        "        cv2.circle(img_matches, center1, 8, color, -1)  # Filled circle\n",
        "        cv2.circle(img2_with_corners, center2, 8, color, -1)  # Filled circle\n",
        "\n",
        "        # Optionally draw lines between matches (comment out if not needed)\n",
        "        # cv2.line(img_matches, center1, (int(x2 + img1.shape[1]), int(y2)), color, 2)\n",
        "\n",
        "    # Combine images for display\n",
        "    img_matches = np.concatenate((img_matches, img2_with_corners), axis=1)\n",
        "\n",
        "    return img_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference\n",
        "inputs = {\n",
        "    'dec1_ids': dec1_ids_np, \n",
        "    'dec1_kps': dec1_kps_np, \n",
        "    'dec1_desc': dec1_desc_np, \n",
        "    'dec1_sc': dec1_sc_np,\n",
        "    'dec2_ids': dec2_ids_np,\n",
        "    'dec2_kps': dec2_kps_np, \n",
        "    'dec2_desc': dec2_desc_np,\n",
        "}\n",
        "outputs = session_match.run(None, inputs)\n",
        "print(outputs[0].shape)\n",
        "print(outputs[1].shape)\n",
        "print(outputs[2].shape)\n",
        "# print(outputs[3].shape)\n",
        "mkpts_0 = outputs[1]\n",
        "mkpts_1 = outputs[3]\n",
        "\n",
        "idx = outputs[0].squeeze(1)\n",
        "print(idx[1])\n",
        "target_idx = outputs[2].squeeze(1)\n",
        "print(target_idx[1])\n",
        "\n",
        "# indices = outputs[3].squeeze(1)\n",
        "\n",
        "# ref_pnts = mkpts_0[indices]\n",
        "# target_pnts = mkpts_1[indices]\n",
        "# print(ref_pnts.shape)\n",
        "# print(target_pnts.shape)\n",
        "# H = outputs[3]\n",
        "# H, _ = cv2.findHomography(mkpts_0, mkpts_1, cv2.USAC_MAGSAC, 3.5, maxIters=1_000, confidence=0.999)\n",
        "\n",
        "\n",
        "# print(\"idx: \", idx)\n",
        "# print(\"output[0]: \", outputs[0])\n",
        "# print(\"mkpts_0: \", mkpts_0)\n",
        "# print(\"mkpts_1: \", mkpts_1)\n",
        "\n",
        "\n",
        "\n",
        "canvas = warp_corners_and_draw_matches(mkpts_0, mkpts_1, im1, im2)\n",
        "# canvas = warp_corners_and_draw_matches(ref_pnts, target_pnts, im1, im2, H)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(canvas[..., ::-1]), plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fisheye (Can ignore this for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im1 = cv2.imread('/Users/vaidehi.som/Documents/accelerated_features/frames/fisheye/current_frame_10.jpg')\n",
        "im2 = cv2.imread('/Users/vaidehi.som/Documents/accelerated_features/frames/fisheye/ref_frame_10.jpg')\n",
        "print(im1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# undistoring fisheye image\n",
        "def undistort(image):\n",
        "    # Define your camera matrix 'K' and distortion coefficients 'D'\n",
        "    # These values are usually obtained through calibration and are specific to each camera\n",
        "    K = np.array([[fx, 0, cx],  # fx and cx are the focal length and principal point x-coordinate\n",
        "                [0, fy, cy],  # fy and cy are the focal length and principal point y-coordinate\n",
        "                [0, 0, 1]])   # Standard form for camera matrix\n",
        "    D = np.array([k1, k2, k3, k4])  # Distortion coefficients\n",
        "\n",
        "    # You need to set the dimension of the undistorted image\n",
        "    # Usually, it is good to provide the new dimensions of the image.\n",
        "    dim = (2432, 2160)  # width, height\n",
        "\n",
        "    # Maps the fisheye image to a new image\n",
        "    map1, map2 = cv2.fisheye.initUndistortRectifyMap(K, D, np.eye(3), K, dim, cv2.CV_16SC2)\n",
        "    undistorted_img = cv2.remap(image, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
        "    return undistorted_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im1_gray = prepare_detect_input(im1)\n",
        "im1_gray_np = im1_gray.cpu().detach().numpy()\n",
        "im1_gray_np = undistort(im1_gray_np)\n",
        "\n",
        "inputs = {'img': im1_gray_np}\n",
        "output1 = session_detect.run(None, inputs)\n",
        "\n",
        "print(output1[0].shape)\n",
        "print(output1[1].shape)\n",
        "print(output1[2].shape)\n",
        "\n",
        "\n",
        "# Draw keypoints on the images\n",
        "ref_points = output1[0].squeeze(0)\n",
        "keypoints1 = [cv2.KeyPoint(x=p[0], y=p[1], size=5) for p in ref_points]\n",
        "\n",
        "# Draw keypoints on the image\n",
        "im1_cv = convert_img(im1)\n",
        "im1_keypoints = cv2.drawKeypoints(im1_cv, keypoints1, None, color=(255, 0, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Use Matplotlib to display the image\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(im1_keypoints)\n",
        "plt.axis('off') \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im2_gray = prepare_detect_input(im2)\n",
        "im2_gray_np = im2_gray.cpu().detach().numpy()\n",
        "\n",
        "inputs = {'img': im2_gray_np}\n",
        "output2 = session_detect.run(None, inputs)\n",
        "\n",
        "print(output2[0].shape)\n",
        "print(output2[1].shape)\n",
        "print(output2[2].shape)\n",
        "\n",
        "\n",
        "# Draw keypoints on the images\n",
        "dst_points = output2[0].squeeze(0)\n",
        "keypoints2 = [cv2.KeyPoint(x=p[0], y=p[1], size=5) for p in dst_points]\n",
        "\n",
        "# Draw keypoints on the image\n",
        "im2_cv = convert_img(im2)\n",
        "im2_keypoints = cv2.drawKeypoints(im2_cv, keypoints2, None, color=(255, 0, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Use Matplotlib to display the image\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(im2_keypoints)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KM1KQaj9-oOv"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "xfeat",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
